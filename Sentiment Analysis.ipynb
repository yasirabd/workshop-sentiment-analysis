{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modul 3: Sentiment Analysis\n",
    "Sentiment Analysis adalah proses melakukan identifikasi dan ekstrak informasi dari data teks untuk mengetahui sentimen dari produk yang bisa berupa positif, negatif, atau netral. Berikut ini adalah contohnya dari review film Spider-Man: Into the Spider-Verse:\n",
    "\n",
    "* positif: \"Keren banget animasinya, film animasi terbaik yang pernah saya tonton!\"\n",
    "* negatif: \"Biasa aja gak ada seru-serunya, alur ceritanya jelek.\"\n",
    "* netral: \"Pertamax gan!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore Data\n",
    "Dataset yang digunakan adalah data hasil crawling dari Bukalapak, yang setiap datanya sudah terdapat column rate dari 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lakukan load dataset terlebih dahulu\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba kita perhatikan berapa banyak data yang kita dapatkan\n",
    "# TODO: jalankan perintah .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba kita lihat salah satu hasil review\n",
    "df_review[\"review\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu tantangan dalam data kita kali ini adalah banyak bahasa yang tidak lengkap dan tidak baku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba kita lihat berapa banyak jumlah data untuk tiap rate\n",
    "# TODO: gunakan .value_counts() untuk lihat jumlah data tiap rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba kita lihat user unik yang ada\n",
    "# TODO: gunakan .value_counts() untuk lihat jumlah data tiap nama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba kita lihat salah satu user, komentar apa yang dia kemukakan\n",
    "# TODO: tampilkan komentar salah satu dari top user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing\n",
    "Pada tahap ini tujuannya adalah mengubah dataset supaya dapat diolah pada algoritma Machine Learning. \n",
    "\n",
    "Pertama, kita akan mengubah menjadi 2 sentiment dulu, yaitu 'positif' dan 'negatif'.\n",
    "* Untuk sentiment positif, adalah data dengan rate 5, menjadi label 1\n",
    "* Untuk sentiment negatif, adalah data dengan rate 1-4, menjadi label 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita lihat dulu distribusi label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "stars_histogram = df_review[\"rate\"].value_counts().sort_index()\n",
    "\n",
    "stars_histogram.plot(kind=\"bar\", width=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengubah rate\n",
    "label = []\n",
    "# TODO: buat perulangan for untuk ganti label\n",
    "\n",
    "df_review[\"label\"] = label\n",
    "df_review = df_review.drop(columns=['rate'])\n",
    "df_review.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek lagi data\n",
    "df_review['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita lihat lagi distribusi label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "stars_histogram = df_review[\"label\"].value_counts().sort_index()\n",
    "\n",
    "stars_histogram.plot(kind=\"bar\", width=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Data\n",
    "Hapus tanda baca, lower text, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "\n",
    "def cleansing(data):\n",
    "    # lower text\n",
    "    # TODO: buat lower teksnya\n",
    "    \n",
    "    # hapus punctuation\n",
    "    # TODO: hapus punctiation\n",
    "    \n",
    "    # remove ASCII dan unicode\n",
    "    data = data.encode('ascii', 'ignore').decode('utf-8')\n",
    "    data = re.sub(r'[^\\x00-\\x7f]',r'', data)\n",
    "    \n",
    "    # remove newline\n",
    "    # TODO: hapus newline\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jalankan cleansing data\n",
    "review = []\n",
    "for index, row in df_review.iterrows():\n",
    "    review.append(cleansing(row[\"review\"]))\n",
    "    \n",
    "df_review[\"review\"] = review\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghapus Stopwords\n",
    "Berikutnya, kita akan menghapus stopwords menggunakan library Sastrawi. Stopwords adalah kata-kata yang tidak memiliki makna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sastrawi terlebih dahulu dengan pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    " \n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    " \n",
    "# Contoh\n",
    "kalimat = 'Dengan Menggunakan Python dan Library Sastrawi saya dapat melakukan proses Stopword Removal'\n",
    "stop = stopword.remove(kalimat)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lakukan pada data kita\n",
    "\n",
    "review = []\n",
    "# TODO: lakukan hapus stopwords pada datamu\n",
    "\n",
    "df_review[\"review\"] = review\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melakukan Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# contoh\n",
    "kalimat = 'Valentino Rossi tampak sangat menyesal setelah terjatuh pada lap terakhir MotoGP Prancis 2017'\n",
    "katadasar = stemmer.stem(kalimat)\n",
    " \n",
    "print(katadasar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementasi pada data kita\n",
    "review = []\n",
    "# TODO: lakukan stemming pada datamu\n",
    "\n",
    "df_review[\"review\"] = review\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sekarang coba kita cek frekuensi kemunculan untuk tiap kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Column Dataframe Tidak dipakai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = df_review.copy()\n",
    "df_preprocessed = df_preprocessed.drop(columns=['id_review', 'name'])\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagi menjadi data train dan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sklearn terlebih dahulu dengan pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: bagi dataset menjadi data training dan data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lihatlah perbandingannya untuk data train pada sentiment positive dan negative\n",
    "print(train['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena jumlah review sentiment positive dan negative tidak seimbang pada data training, kita akan buat dataset menjadi seimbang dengan bootstraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap\n",
    "t_1 = train[train['label']==0].sample(2000,replace=True)\n",
    "t_2 = train[train['label']==1].sample(2000,replace=True)\n",
    "training_bs = pd.concat([t_1, t_2])\n",
    "\n",
    "print(train.shape)\n",
    "print(training_bs.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# sanity check \n",
    "df_review.shape[0] == (train.shape[0] + test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_bs['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install terlebih dahulu dengan pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Polarity == 0 negative\n",
    "train_s0 = training_bs[training_bs[\"label\"] == 0]\n",
    "all_text_s0 = ' '.join(word for word in train_s0[\"review\"])\n",
    "wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "\n",
    "# Polarity == 1 positive\n",
    "train_s1 = training_bs[training_bs[\"label\"] == 1]\n",
    "all_text_s1 = ' '.join(word for word in train_s1[\"review\"])\n",
    "wordcloud = WordCloud(width=1000, height=1000, colormap='Blues', background_color='white', mode='RGBA').generate(all_text_s1)\n",
    "plt.figure( figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita juga bisa cek top 20 kata untuk masing-masing sentiment\n",
    "from collections import Counter\n",
    "\n",
    "# TODO: tampilkan top 50 kata pada masing-masing sentiment\n",
    "\n",
    "print(counter_s0)\n",
    "print()\n",
    "print(counter_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Feature extraction merupakan sebuah step untuk mentransformasi teks kedalam angka atau yang bisa disebut feature representation. Terdapat beragam teknik untuk merubah teks kedalam array, tapi pada dasarnya kata kata unik dalam corpus itu yang menjadi featurenya. Feature extraction yang umum digunakan dalam sklearn adalah Count Vectorizer dan TF-IDF.\n",
    "\n",
    "\n",
    "Count Vectorizer ini mudahnya menghitung semua kejadian kata dalam satu dokumen sedangkan tf-idf memberikan bobot masing masing kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install terlebih dahulu sklearn dengan pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Pusing satu kali',\n",
    "    'Pusing dua kali',\n",
    "    'Pusing tiga kali',\n",
    "    'Pusing lagi']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# contoh\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementasi pada dokumen kita\n",
    "# TODO: lakukan fitur ekstraksi pada data training dan data testing\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping All Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # cleansing data\n",
    "    data = cleansing(data)\n",
    "    \n",
    "    # hapus stopwords\n",
    "    factory = StopWordRemoverFactory()\n",
    "    stopword = factory.create_stop_word_remover()\n",
    "    data = stopword.remove(data)\n",
    "    \n",
    "    # stemming\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    data = stemmer.stem(data)\n",
    "    \n",
    "    # count vectorizer\n",
    "    data = vectorizer.transform([data])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Pada algoritma Machine Learning yang digunakan adalah Naive Bayes Classifier.\n",
    "\n",
    "Naive Bayes classifier merupakan sebuh metode klasifikasi dengan probabilitas sederhana yang mengaplikasikan teorema Bayes dengan tidak ketergantungan (independen) yang tinggi.Model klasifikasi naive Bayes menghitung probabilitas posterior suatu kelas, berdasarkan pada distribusi kata-kata dalam dokumen. Klasifikasi naive Bayes dibangun oleh data pelatihan untuk memperkirakan probabilitas dari setiap kategori yang terdapat pada ciri dokumen yang diuji. Sistem akan dilatih dengan menggunakan data baru (data latih dan data uji) dan selanjutnya diberi tugas untuk menebak nilai fungsi target dari data tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# TODO: buat classifier dan lakukan training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Dalam metode Evaluation menggunakan Confusion Matrix. Confusion matrix adalah matrix yang merepresentasikan hasil klasifikasi biner pada suatu dataset. Terdapat beberapa rumus umum yang dapat digunakan untuk menghitung performa klasifikasi. Hasil dari nilai accuracy, precision dan recall bisa ditampilkan dalam persentase.\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision | Accuracy = \\frac{TP}{(TP + FP)}.\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall|Sensitivity|True Positive Rate = \\frac{TP}{(TP + FN)}.\n",
    "\\end{equation*}\n",
    "\n",
    "- **TP:** true positive\n",
    "- **FP:** false positive\n",
    "- **FN:** false negative\n",
    "\n",
    "\n",
    "It is convenient to combine precision and recall into a single metric called *F1 score.*\n",
    "\n",
    "\\begin{equation*}\n",
    "F_1 = \\frac{TP}{\\frac{1}{precision} + \\frac{1}{recall}} = 2 \\times \\frac{precision \\times recall}{precision + recall} = \\frac{TP}{TP + \\frac{FN + FP}{2}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lakukan prediksi pada data test\n",
    "# TODO: lakukan prediksi pada data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library evaluation\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score\n",
    "f1_score(test['label'], predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision score\n",
    "precision_score(test['label'], predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall score\n",
    "recall_score(test['label'], predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(test['label'], predict).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita coba lakukan prediksi sentiment pada suatu data baru yang kita buat sendiri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_positif = \"barangnya bagus banget\"\n",
    "review_negatif = \"barang jelek pelayanan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pda hasil, jika menghasilkan 1, maka reviewnya adalah positif. Sebalinyak jika menghasilkan 0, maka reviewnya adalah negatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: lakukan predict pada data review_positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: lakukan predict pada data review_negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
